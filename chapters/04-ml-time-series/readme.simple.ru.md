# Глава 4: Машинное обучение для торговли (Простое объяснение)

## Что такое машинное обучение?

Представь, что ты учишь собаку приносить мячик. Сначала она не понимает, что делать. Но ты показываешь ей много раз: "принеси мячик — получишь вкусняшку". После множества попыток собака **сама** понимает, что нужно делать.

**Машинное обучение** — это когда мы учим компьютер так же, как собаку. Только вместо мячика — данные о ценах, а вместо вкусняшки — правильные ответы.

---

## Почему это сложно с деньгами?

### Представь школьную столовую

Допустим, ты хочешь угадать, какой будет очередь в столовой завтра.

**Проблема 1: Всё меняется**
- В понедельник давали пиццу — очередь огромная
- Во вторник — каша — почти пусто
- А в среду пришёл новый повар!

Рынок такой же — правила постоянно меняются. То, что работало вчера, может не работать завтра.

**Проблема 2: Много шума**
Представь, что ты пытаешься услышать друга на дискотеке. Музыка орёт, все кричат — это **шум**. А голос друга — это **сигнал**.

На рынке так же: большинство движений цены — это шум. Найти настоящий сигнал очень сложно!

**Проблема 3: Нельзя подглядывать в будущее**
Это как если бы ты готовился к контрольной, имея ответы. На контрольной ты получишь 5, но в жизни ответов не будет!

Когда мы учим компьютер на исторических данных, нужно притворяться, что будущего мы не знаем.

---

## Как правильно учить компьютер торговать?

### Метод "Шаг за шагом" (Walk-Forward)

Представь, что ты учишься кататься на велосипеде:
1. Сначала учишься с поддерживающими колёсами (тренировка)
2. Потом пробуешь без них во дворе (тест)
3. Если упал — снова учишься
4. Потом пробуешь на улице

```rust
/// Валидатор "шаг за шагом"
/// Как учитель, который сначала объясняет, потом проверяет
pub struct WalkForwardValidator {
    /// Сколько раз будем проверять
    num_tests: usize,
    /// Перерыв между учёбой и проверкой (чтобы не подглядывать)
    break_days: usize,
}

impl WalkForwardValidator {
    pub fn new(num_tests: usize, break_days: usize) -> Self {
        Self { num_tests, break_days }
    }

    /// Делим данные на части: учёба → перерыв → проверка
    pub fn split(&self, total_days: usize) -> Vec<(usize, usize, usize, usize)> {
        let mut splits = Vec::new();
        let chunk_size = total_days / (self.num_tests + 1);

        for i in 0..self.num_tests {
            let learn_end = (i + 1) * chunk_size;
            let test_start = learn_end + self.break_days;
            let test_end = test_start + chunk_size;

            // (начало учёбы, конец учёбы, начало теста, конец теста)
            splits.push((0, learn_end, test_start, test_end.min(total_days)));
        }

        splits
    }
}

// Пример использования:
// Представь, что у нас данные за 100 дней
// validator.split(100) разделит их на части:
// 1. Учимся на днях 0-25, проверяем на 30-55
// 2. Учимся на днях 0-50, проверяем на 55-80
// и так далее...
```

---

## Метод трёх барьеров

### Аналогия: Игра с мячом

Представь, что ты подбрасываешь мяч в комнате:
- **Потолок** — если мяч ударится о потолок, ты выиграл! (цена выросла достаточно)
- **Пол** — если упадёт на пол, проиграл... (цена упала)
- **Время** — если за 10 секунд никуда не ударился — ничья (цена почти не изменилась)

```rust
/// Три барьера: победа, поражение или ничья
#[derive(Debug, Clone, Copy)]
pub enum GameResult {
    Win,   // Цена выросла достаточно (мяч ударил потолок)
    Lose,  // Цена упала слишком сильно (мяч упал на пол)
    Draw,  // Время вышло (мяч завис в воздухе)
}

/// Игра в "три барьера"
pub struct ThreeBarriersGame {
    /// Насколько цена должна вырасти для победы (например, 2%)
    ceiling_percent: f64,
    /// Насколько цена может упасть до поражения (например, -2%)
    floor_percent: f64,
    /// Сколько времени ждём
    wait_time: usize,
}

impl ThreeBarriersGame {
    pub fn new(win_threshold: f64, lose_threshold: f64, max_wait: usize) -> Self {
        Self {
            ceiling_percent: win_threshold,
            floor_percent: -lose_threshold.abs(),
            wait_time: max_wait,
        }
    }

    /// Играем! Смотрим, что случится с ценой
    pub fn play(&self, prices: &[f64], start: usize) -> Option<GameResult> {
        // Нужно достаточно данных впереди
        if start + self.wait_time >= prices.len() {
            return None;
        }

        let start_price = prices[start];

        // Смотрим каждый следующий день
        for day in 1..=self.wait_time {
            let current_price = prices[start + day];
            let change = (current_price - start_price) / start_price * 100.0;

            // Ударились о потолок?
            if change >= self.ceiling_percent {
                return Some(GameResult::Win);
            }

            // Упали на пол?
            if change <= self.floor_percent {
                return Some(GameResult::Lose);
            }
        }

        // Время вышло — ничья
        Some(GameResult::Draw)
    }
}

// Пример:
// let game = ThreeBarriersGame::new(2.0, 2.0, 10);
// Если цена вырастет на 2% за 10 дней — Win
// Если упадёт на 2% — Lose
// Если останется в коридоре — Draw
```

---

## Индикаторы: помощники для принятия решений

### RSI — Индикатор "усталости"

Представь бегуна на длинной дистанции:
- Если он бежал очень быстро долгое время, скорее всего **устанет** и замедлится
- Если долго еле плёлся, возможно **отдохнул** и ускорится

RSI показывает, насколько "устал" рынок от роста или падения.

```rust
/// Индикатор усталости рынка (RSI)
/// Как датчик уровня энергии у спортсмена
pub struct FatigueIndicator {
    /// Сколько дней смотрим назад
    lookback_days: usize,
}

impl FatigueIndicator {
    pub fn new(days: usize) -> Self {
        Self { lookback_days: days }
    }

    /// Считаем уровень "усталости" от 0 до 100
    /// 0-30 = очень устал от падения (может пора расти?)
    /// 70-100 = очень устал от роста (может пора падать?)
    /// 30-70 = нормальное состояние
    pub fn calculate(&self, prices: &[f64]) -> Option<f64> {
        if prices.len() < self.lookback_days + 1 {
            return None;
        }

        let recent_prices = &prices[prices.len() - self.lookback_days - 1..];

        // Считаем, сколько было подъёмов и спусков
        let mut energy_gained = 0.0;  // Энергия от роста
        let mut energy_spent = 0.0;   // Энергия от падения

        for i in 1..recent_prices.len() {
            let change = recent_prices[i] - recent_prices[i - 1];
            if change > 0.0 {
                energy_gained += change;
            } else {
                energy_spent += change.abs();
            }
        }

        // Средняя энергия
        let avg_gain = energy_gained / self.lookback_days as f64;
        let avg_loss = energy_spent / self.lookback_days as f64;

        // Если совсем не было падений — максимальная усталость от роста
        if avg_loss == 0.0 {
            return Some(100.0);
        }

        // Формула RSI
        let strength = avg_gain / avg_loss;
        Some(100.0 - 100.0 / (1.0 + strength))
    }

    /// Простое объяснение результата
    pub fn explain(rsi: f64) -> &'static str {
        if rsi < 30.0 {
            "Рынок очень устал падать. Возможно, скоро начнёт расти!"
        } else if rsi > 70.0 {
            "Рынок очень устал расти. Возможно, скоро начнёт падать!"
        } else {
            "Рынок в нормальном состоянии. Ничего особенного."
        }
    }
}
```

### Скользящая средняя — "сглаженная" цена

Представь, что ты смотришь на погоду:
- Вчера было +10°C
- Сегодня +20°C
- Значит ли это, что погода резко изменилась?

**Скользящая средняя** — это как средняя температура за неделю. Она показывает **тренд**, а не случайные скачки.

```rust
/// Скользящая средняя — "сглаживатель шума"
/// Как очки, которые убирают рябь с картинки
pub struct SmoothingGlasses {
    /// Сколько дней усреднять
    smooth_period: usize,
}

impl SmoothingGlasses {
    pub fn new(period: usize) -> Self {
        Self { smooth_period: period }
    }

    /// Надеваем "очки" и смотрим на цены
    /// Возвращаем сглаженную цену
    pub fn look(&self, prices: &[f64]) -> Option<f64> {
        if prices.len() < self.smooth_period {
            return None;
        }

        // Берём последние N дней и усредняем
        let recent = &prices[prices.len() - self.smooth_period..];
        let sum: f64 = recent.iter().sum();
        Some(sum / self.smooth_period as f64)
    }

    /// Сравниваем текущую цену со сглаженной
    pub fn compare_with_current(&self, prices: &[f64]) -> Option<String> {
        let smooth = self.look(prices)?;
        let current = *prices.last()?;

        if current > smooth * 1.02 {
            Some(String::from("Цена ВЫШЕ средней! Может быть восходящий тренд."))
        } else if current < smooth * 0.98 {
            Some(String::from("Цена НИЖЕ средней! Может быть нисходящий тренд."))
        } else {
            Some(String::from("Цена около средней. Тренд неясен."))
        }
    }
}

// Пример:
// Цены: [100, 102, 98, 105, 103, 108, 110]
// Средняя за 5 дней: (98+105+103+108+110)/5 = 104.8
// Текущая цена 110 > 104.8 → возможно, растём!
```

---

## Нейронные сети: мозг компьютера

### LSTM — память рыбки Дори

Помнишь мультик "В поисках Немо"? Рыбка Дори забывала всё через секунду.

Обычные нейронные сети — как Дори. Они забывают, что было раньше.

**LSTM** (Long Short-Term Memory) — это нейросеть с хорошей памятью. Она помнит важные вещи из прошлого!

```rust
/// Клетка памяти LSTM
/// Как ячейка в твоём мозге, которая решает:
/// - Что запомнить?
/// - Что забыть?
/// - Что рассказать дальше?
pub struct MemoryCell {
    /// Ворота забывания: что выкинуть из памяти?
    forget_gate: f64,
    /// Ворота входа: что новое запомнить?
    input_gate: f64,
    /// Ворота выхода: что рассказать?
    output_gate: f64,
    /// Долгосрочная память
    long_memory: f64,
    /// Краткосрочная память (что помню прямо сейчас)
    short_memory: f64,
}

impl MemoryCell {
    pub fn new() -> Self {
        Self {
            forget_gate: 0.0,
            input_gate: 0.0,
            output_gate: 0.0,
            long_memory: 0.0,
            short_memory: 0.0,
        }
    }

    /// Обработка новой информации
    /// Как когда тебе рассказывают что-то новое
    pub fn process(&mut self, new_info: f64) -> f64 {
        // 1. Решаем, что забыть из старой памяти
        // (например, забыть что ел на завтрак неделю назад)
        self.forget_gate = Self::think(new_info + self.short_memory);

        // 2. Решаем, что из нового важно запомнить
        // (например, запомнить день рождения друга)
        self.input_gate = Self::think(new_info);

        // 3. Обновляем долгосрочную память
        let new_memory_candidate = (new_info * 2.0 - 1.0).tanh(); // от -1 до 1
        self.long_memory = self.forget_gate * self.long_memory
                         + self.input_gate * new_memory_candidate;

        // 4. Решаем, что сейчас важно (выход)
        self.output_gate = Self::think(new_info + self.long_memory);

        // 5. Обновляем краткосрочную память
        self.short_memory = self.output_gate * self.long_memory.tanh();

        self.short_memory
    }

    /// Функция "подумать" — превращает число в вероятность (0 до 1)
    fn think(x: f64) -> f64 {
        1.0 / (1.0 + (-x).exp())
    }
}

// Пример использования:
// let mut brain = MemoryCell::new();
// brain.process(100.0);  // Цена 100
// brain.process(105.0);  // Цена выросла до 105
// brain.process(103.0);  // Немного упала до 103
// Теперь brain помнит: был рост, потом небольшое падение
```

---

## Обучение с подкреплением: как научить робота торговать

### Аналогия: Обучение через игру

Представь, что ты учишься играть в компьютерную игру:
- Набрал очки — приятно! (награда)
- Потерял жизнь — обидно! (штраф)
- Со временем понимаешь, как играть лучше

```rust
/// Торговый бот-ученик
/// Как ребёнок, который учится принимать решения
pub struct TradingStudent {
    /// Сколько денег в кармане (относительно начала)
    wallet: f64,
    /// Что сейчас держим (-1 = продали, 0 = ничего, 1 = купили)
    holding: f64,
    /// Таблица опыта: ситуация -> действие -> насколько хорошо
    experience: std::collections::HashMap<String, [f64; 3]>,
    /// Как быстро учимся (0.1 = медленно, 0.9 = быстро)
    learning_speed: f64,
    /// Как часто пробуем новое (0.1 = редко, 0.9 = часто)
    curiosity: f64,
}

impl TradingStudent {
    pub fn new() -> Self {
        Self {
            wallet: 1.0,
            holding: 0.0,
            experience: std::collections::HashMap::new(),
            learning_speed: 0.1,
            curiosity: 0.2,
        }
    }

    /// Смотрим на ситуацию и решаем, что делать
    pub fn decide(&self, situation: &str) -> Action {
        // Иногда пробуем случайное действие (исследуем)
        if Self::random() < self.curiosity {
            return match (Self::random() * 3.0) as usize {
                0 => Action::Sell,
                1 => Action::Wait,
                _ => Action::Buy,
            };
        }

        // Обычно делаем то, что раньше приносило больше очков
        let scores = self.experience
            .get(situation)
            .unwrap_or(&[0.0, 0.0, 0.0]);

        if scores[0] > scores[1] && scores[0] > scores[2] {
            Action::Sell
        } else if scores[2] > scores[0] && scores[2] > scores[1] {
            Action::Buy
        } else {
            Action::Wait
        }
    }

    /// Учимся на результате
    pub fn learn(&mut self, situation: &str, action: Action, reward: f64) {
        let action_idx = match action {
            Action::Sell => 0,
            Action::Wait => 1,
            Action::Buy => 2,
        };

        let scores = self.experience
            .entry(situation.to_string())
            .or_insert([0.0, 0.0, 0.0]);

        // Обновляем оценку действия
        // Новая оценка = старая + скорость_обучения * (награда - старая_оценка)
        let old_score = scores[action_idx];
        scores[action_idx] = old_score + self.learning_speed * (reward - old_score);
    }

    fn random() -> f64 {
        use std::time::{SystemTime, UNIX_EPOCH};
        let nanos = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .subsec_nanos();
        (nanos as f64) / (u32::MAX as f64)
    }
}

#[derive(Debug, Clone, Copy)]
pub enum Action {
    Sell,  // Продать (думаем, что цена упадёт)
    Wait,  // Ничего не делать
    Buy,   // Купить (думаем, что цена вырастет)
}

// Как это работает:
// 1. Бот смотрит на ситуацию (например, "RSI низкий")
// 2. Решает: купить, продать или ждать
// 3. Получает награду или штраф
// 4. Запоминает: "когда RSI низкий, лучше покупать!"
```

---

## Как измерить успех?

### Коэффициент Шарпа: награда за риск

Представь двух школьников:
- Петя получает пятёрки, но иногда и двойки (большой разброс)
- Маша получает четвёрки стабильно (маленький разброс)

Кто учится лучше? Зависит от того, как мы ценим стабильность!

**Коэффициент Шарпа** показывает: сколько прибыли мы получаем на единицу риска.

```rust
/// Калькулятор успеха
pub struct SuccessCalculator;

impl SuccessCalculator {
    /// Коэффициент Шарпа
    /// Чем выше — тем лучше соотношение прибыль/риск
    /// > 1.0 = хорошо
    /// > 2.0 = отлично!
    /// < 0 = плохо, теряем деньги
    pub fn sharpe_ratio(daily_returns: &[f64]) -> f64 {
        if daily_returns.is_empty() {
            return 0.0;
        }

        // Средняя дневная прибыль
        let avg: f64 = daily_returns.iter().sum::<f64>() / daily_returns.len() as f64;

        // Насколько прибыль "прыгает" (стандартное отклонение)
        let variance: f64 = daily_returns
            .iter()
            .map(|r| (r - avg).powi(2))
            .sum::<f64>() / daily_returns.len() as f64;
        let volatility = variance.sqrt();

        if volatility == 0.0 {
            return 0.0;
        }

        // Приводим к годовому значению (252 торговых дня)
        avg / volatility * (252.0_f64).sqrt()
    }

    /// Объяснение результата
    pub fn explain_sharpe(ratio: f64) -> &'static str {
        if ratio < 0.0 {
            "Ой-ой! Мы теряем деньги. Нужно что-то менять!"
        } else if ratio < 0.5 {
            "Так себе. Прибыль есть, но риск высокий."
        } else if ratio < 1.0 {
            "Неплохо! Прибыль примерно равна риску."
        } else if ratio < 2.0 {
            "Хорошо! Прибыль больше, чем риск."
        } else {
            "Отлично! Высокая прибыль при низком риске!"
        }
    }

    /// Процент выигрышей
    pub fn win_rate(returns: &[f64]) -> f64 {
        if returns.is_empty() {
            return 0.0;
        }

        let wins = returns.iter().filter(|&&r| r > 0.0).count();
        wins as f64 / returns.len() as f64 * 100.0
    }

    /// Максимальная просадка (самый большой "провал")
    /// Как самая длинная серия поражений в игре
    pub fn max_drawdown(portfolio_values: &[f64]) -> f64 {
        if portfolio_values.is_empty() {
            return 0.0;
        }

        let mut peak = portfolio_values[0];
        let mut max_drop = 0.0;

        for &value in portfolio_values {
            if value > peak {
                peak = value;
            }
            let current_drop = (peak - value) / peak * 100.0;
            if current_drop > max_drop {
                max_drop = current_drop;
            }
        }

        max_drop
    }
}

// Пример:
// Дневные доходности: [0.01, -0.005, 0.02, -0.01, 0.015, 0.008]
// Средняя: 0.63%
// Волатильность: 1.1%
// Sharpe = 0.63 / 1.1 * sqrt(252) ≈ 9.1 (очень хорошо!)
```

---

## Главные правила машинного обучения для трейдинга

### 1. Не подглядывать в будущее!
Как на экзамене — нельзя смотреть ответы заранее.

### 2. Данные постоянно меняются
Рынок сегодня — не такой как вчера. Нужно постоянно переучиваться.

### 3. Большинство движений — шум
Не каждое движение цены что-то значит. Нужно искать настоящие сигналы.

### 4. Простое лучше сложного
Часто простая стратегия работает лучше, чем супер-умная нейросеть.

### 5. Проверяй на новых данных
Если стратегия работает только на старых данных — она бесполезна!

---

## Что дальше?

После этой главы ты понимаешь:
- Как компьютер "учится" торговать
- Почему нельзя подглядывать в будущее
- Что такое индикаторы (RSI, скользящие средние)
- Как работает память нейросетей (LSTM)
- Как обучать торгового робота через игру
- Как измерять успех (коэффициент Шарпа)

В следующих главах мы узнаем, как сделать всё это **очень быстрым** — ведь на бирже каждая миллисекунда важна!

---

## Словарик

| Термин | Простое объяснение |
|--------|-------------------|
| **Машинное обучение** | Обучение компьютера на примерах |
| **Нейросеть** | Программа, похожая на мозг |
| **LSTM** | Нейросеть с хорошей памятью |
| **RSI** | Индикатор "усталости" рынка |
| **Скользящая средняя** | "Сглаженная" цена |
| **Коэффициент Шарпа** | Награда за риск |
| **Walk-Forward** | Проверка "шаг за шагом" |
| **Reinforcement Learning** | Обучение через награды и штрафы |
